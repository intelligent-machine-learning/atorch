## training args
output_dir: !ENV ${OUTPUT_DIR}
overwrite_output_dir: True
# resume_from_checkpoint: !ENV ${OUTPUT_DIR}
# flash_checkpoint: True
do_train: True
do_eval: True
distributed_type: "megatron"
num_train_epochs: 6
block_size: 512
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
preprocessing_num_workers: 6
learning_rate: 2.0e-5
weight_decay: 0.0
warmup_ratio: 0.03
seed: 42
max_grad_norm: 0
bf16: True
save_strategy: "steps"
save_steps: 1000
save_total_limit: 3
evaluation_strategy: "steps"
eval_steps: 2000
logging_strategy: "steps"
logging_steps: 1
logging_nan_inf_filter: False
# log_params_std: True
# log_grad_diff_for_debug: True
tensorboard_dir: !ENV ${OUTPUT_DIR}/runs/${TIME_STAMP}
dataloader_num_workers: 0
gradient_checkpointing: True

## data args
data_path: &data_path /hetero_infer/jinshi.cl/code/Megatron-LM-core_r0.6.0/wikitext-2-raw-v1/gpt2_tokenized_train_text_document

## profiling args
# profiler_type: nv
# profiler_file_path: !ENV ${OUTPUT_DIR}/profiler_output
# profiler_schedule_wait: 1
# profiler_schedule_warmup: 1
# profiler_schedule_active: 1
# profiler_schedule_repeat: 1
# profiler_schedule_skip_first: 20

extra_configs:
  model_type_name: "gpt2"
  num_layers: 12
  hidden_size: 768
  num_attention_heads: 12
  max_position_embeddings: 4096
  position_embedding_type: "rope"
  make_vocab_size_divisible_by: 1
  norm_epsilon: 1.0e-5
  normalization: "RMSNorm"
  untie_embeddings_and_output_weights: True
  use_flash_attn: True
  tokenizer_type: "GPT2BPETokenizer"
  vocab_file: /hetero_infer/public/pretrained_models/gpt/gpt2/vocab.json
  merge_file: /hetero_infer/public/pretrained_models/gpt/gpt2/merges.txt
  optimizer: "adam"
  attention_dropout: 0.0
  hidden_dropout: 0.0
  weight_decay: 1.0e-1
  clip_grad: 1.0
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-8
  pretraining_flag: False
  use_mcore_models: True
  transformer_impl: "transformer_engine"
  micro_batch_size: 2
  global_batch_size: 16
  add_bias_linear: False
  bias_gelu_fusion: False
  recompute_activations: True
  recompute_granularity: "selective"
  train_iters: 10000
  eval_iters: 50
  tensor_model_parallel_size: 2
  pipeline_model_parallel_size: 2
  sequence_parallel: True
  distributed_backend: "nccl"
  use_distributed_optimizer: True
  log_timers_to_tensorboard: True
  log_validation_ppl_to_tensorboard: True
  log_memory_to_tensorboard: True
  log_throughput: True
  log_params_norm: True
  seed: 1403
  init_method_std: 0.02
  lr: 3.0e-5
  min_lr: 3.0e-6
  lr_ecay_style: "cosine"
  lr_warmup_fraction: 0.1
  data_path: [*data_path]
  split: "949,50,1"
  data_cache_path: !ENV ${OUTPUT_DIR}/data_cache
  seq_length: 4096
  num_workers: 0
